{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "colab": {
      "name": "KAIST AI605 Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.7.11 64-bit ('egyun-nlp': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "interpreter": {
      "hash": "079e1d936214edc7fbe1e2fed26ad4d929fce657ec3c52ac3aa017c206c26a5b"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KAIST AI605 Assignment 1: Text Classification"
      ],
      "metadata": {
        "id": "mbGnNWI1lRy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment\n",
        "You will only use Python 3.7 and PyTorch 1.9, which is already available on Colab:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "source": [
        "from platform import python_version\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"python {python_version()}\")\n",
        "print(f\"torch {torch.__version__}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.7.11\n",
            "torch 1.9.0\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYxEp1XMpxem",
        "outputId": "f9954e7c-f362-40d5-af8e-d37856b71ed9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Limitations of Vanilla RNNs\n",
        "In Lecture 02, we saw that a multi-layer perceptron (MLP) without activation function is equivalent to a single linear transformation with respect to the inputs. One can define a vanilla recurrent neural network without activation as, given inputs $\\textbf{x}_1 \\dots \\textbf{x}_T$, the outputs $\\textbf{h}_t$ is obtained by\n",
        "$$\\textbf{h}_t = \\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b},$$\n",
        "where $\\textbf{V}, \\textbf{U}, \\textbf{b}$ are trainable weights."
      ],
      "metadata": {
        "id": "sB7xyzIgnnkA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 1.1** *(2 point)* Show that such recurrent neural network (RNN) without activation function is equivalent to a single linear transformation with respect to the inputs, which means each $\\textbf{h}_t$ is a linear combination of the inputs."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's substitute $\\textbf{h}_t$ with the fact that $\\textbf{h}_{t-1} = \\textbf{V}\\textbf{h}_{t-2} + \\textbf{U}\\textbf{x}_{t-1} + \\textbf{b}$.\n",
        "Then we get,\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        \\textbf{h}_t\n",
        "        &= \\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b} \\\\\n",
        "        &= \\textbf{V}(\\textbf{V}\\textbf{h}_{t-2} + \\textbf{U}\\textbf{x}_{t-1} + \\textbf{b}) + \\textbf{U}\\textbf{x}_t + \\textbf{b} \\\\\n",
        "        & \\qquad\\qquad\\qquad\\qquad\\vdots \\\\\n",
        "        &= \\textbf{V}(\\textbf{V}( \\cdots \\textbf{V}(\\textbf{U}\\textbf{x}_{1} + \\textbf{b}) + \\textbf{U}\\textbf{x}_{2} + \\textbf{b}) \\cdots) + \\textbf{U}\\textbf{x}_t + \\textbf{b} \\\\\n",
        "        &= \\textbf{V}^{t-1}\\textbf{U}\\textbf{x}_1 + \\textbf{V}^{t-2}\\textbf{U}\\textbf{x}_2 + \\cdots + \\textbf{V}\\textbf{U}\\textbf{x}_{t-1} + \\textbf{U}\\textbf{x}_t + \\left(\\sum_{i=1}^t\\textbf{V}^{t-1}\\right)\\textbf{b} \\\\\n",
        "        &= \\textbf{W}_{t-1}\\textbf{x}_1 + \\textbf{W}_{t-2}\\textbf{x}_2 + \\cdots + \\textbf{W}_1\\textbf{x}_{t-1} + \\textbf{W}_0\\textbf{x}_t + \\textbf{c}_t\n",
        "    \\end{align*}\n",
        "$$\n",
        "where $\\textbf{W}_t = \\textbf{V}^t\\textbf{U}$ and $\\textbf{c}_t = \\left(\\sum_{i=1}^t\\textbf{V}^{t-1}\\right)\\textbf{b}$.\n",
        "And this is the single linear transformation with respect to the inputs $\\textbf{x}_1, \\cdots \\textbf{x}_t$."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Lecture 05 and 06, we will see how RNNs can model non-linearity via activation function, but they still suffer from exploding or vanishing gradients. We can mathematically show that, if the recurrent relation is\n",
        "$$ \\textbf{h}_t = \\sigma (\\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b}) $$\n",
        "then\n",
        "$$ \\frac{\\partial \\textbf{h}_t}{\\partial \\textbf{h}_{t-1}} = \\text{diag}(\\sigma' (\\textbf{V}\\textbf{h}_{t-1} + \\textbf{U}\\textbf{x}_t + \\textbf{b}))\\textbf{V}$$\n",
        "so\n",
        "$$\\frac{\\partial \\textbf{h}_T}{\\partial \\textbf{h}_1} \\propto \\textbf{V}^{T-1}$$\n",
        "which means this term will be very close to zero if the norm of $\\bf{V}$ is smaller than 1 and really big otherwise."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 1.2** *(2 points)* Explain how exploding gradient can be mitigated if we use gradient clipping."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As given above, the gradient of $\\textbf{h}_T$ with respect to $\\textbf{h}_1$ is proportional to $\\textbf{V}^{T-1}$, which is\n",
        "$$\n",
        "    \\textbf{g}_T = \\frac{\\partial \\textbf{h}_T}{\\partial \\textbf{h}_1} \\propto \\textbf{V}^{T-1}.\n",
        "$$\n",
        "This means that if the norm of $\\textbf{V}$ is larger than $1$, the gradient increases exponentially as $T$ increases, which leads to the exploding of the gradient. \\\n",
        "By using gradient clipping, the clipped gradient becomes\n",
        "$$\n",
        "    \\hat{\\textbf{g}}_T = \\begin{cases}\n",
        "        c \\times \\frac{\\textbf{g}_T}{\\left\\Vert\\textbf{g}_T\\right\\Vert} & \\text{ if } \\left\\Vert\\textbf{g}_T\\right\\Vert > c \\\\\n",
        "        \\textbf{g}_T & \\text{otherwise}\n",
        "    \\end{cases}\n",
        "$$\n",
        "where $c$ is the positive constant.\n",
        "Then the norm of the gradient is bounded by $c$, which is $\\left\\Vert\\hat{\\textbf{g}}_T\\right\\Vert \\le c$. \\\n",
        "So, the gradient clipping upper bounds the norm of the gradient by $c$, which prevents the gradient from exploding."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 1.3** *(2 points)* Explain how vanishing gradient can be mitigated if we use LSTM. See the Lecture 05 and 06 slides for the definition of LSTM."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cell state of LSTM is\n",
        "$$\n",
        "    \\textbf{c}_t = \\textbf{f}_t \\circ \\textbf{c}_{t-1} + \\textbf{i}_t \\circ \\tilde{\\textbf{c}}_t,\n",
        "$$\n",
        "the gradient of $\\textbf{c}_T$ with respect to $\\textbf{c}_1$ can be computed as follows:\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        \\textbf{g}_T\n",
        "        &= \\frac{\\partial \\textbf{c}_T}{\\partial \\textbf{c}_1} \\\\\n",
        "        &= \\frac{\\partial \\textbf{c}_T}{\\partial \\textbf{c}_{T-1}} \\cdot \\frac{\\partial \\textbf{c}_{T-1}}{\\partial \\textbf{c}_{T-2}} \\cdots \\frac{\\partial \\textbf{c}_2}{\\partial \\textbf{c}_{1}} \\\\\n",
        "        &= \\textbf{f}_T \\cdot \\textbf{f}_{T-1} \\cdots \\textbf{f}_{2} \\\\\n",
        "        &= \\prod_{i=2}^T \\textbf{f}_{i},\n",
        "    \\end{align*}\n",
        "$$\n",
        "where $\\textbf{f}_t = \\sigma_g\\left(\\textbf{W}_f\\textbf{x}_t + \\textbf{U}_f\\textbf{h}_{t-1} + \\textbf{b}_f\\right)$ and $\\sigma_g$ is a sigmoid function. \\\n",
        "Since $\\textbf{f}_t$ is the output of the sigmoid function, the value of each element is bounded by $(0, 1)$,\n",
        "and this means the gradient remains if each $\\textbf{f}_t$ for $t = 2, \\cdots, T$ is close to $1$.\n",
        "The difference from RNN is that the gradient of RNN is proportional to the product of the same matrix $\\textbf{V}$ and the gradient of LSTM is the product of the different vectors $\\textbf{f}_t$.\n",
        "Therefore, the gradient of RNN vanishes exponentially, whereas LSTM does not."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Creating Vocabulary from Training Data\n",
        "Creating the vocabulary is the first step for every natural language processing model. In this section, you will use Stanford Sentiment Treebank (SST), a popular dataset for sentiment classification, to create your vocabulary.\n",
        "\n",
        "### Obtaining SST via Hugging Face\n",
        "We will use `datasets` package offered by Hugging Face, which allows us to easily download various language datasets, including Stanford Sentiment Treebank.\n",
        "\n",
        "First, install the package:"
      ],
      "metadata": {
        "id": "S0AmoAT3wA1J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "try:\n",
        "    import datasets\n",
        "except ImportError:\n",
        "    !pip install datasets"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YK0S_VTJxds4",
        "outputId": "d8257280-f4f0-47df-c17b-9e37a5f27d09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then download SST and print the first example:"
      ],
      "metadata": {
        "id": "Y8eWbt2yxb3H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "sst = load_dataset(\"sst\", \"default\", cache_dir=\"sst\")\n",
        "train0 = sst[\"train\"][0]\n",
        "print(f\"Sentence: \\\"{train0['sentence']}\\\"\")\n",
        "print(f\"Label:    {train0['label']:.6f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset sst (sst/sst/default/1.0.0/b8a7889ef01c5d3ae8c379b84cc4080f8aad3ac2bc538701cbe0ac6416fb76ff)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: \"The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"\n",
            "Label:    0.694440\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6drFIvgxxjgI",
        "outputId": "d903a062-ac0f-4da1-ec14-efd1d533c4f4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that each `label` is a score between 0 and 1. You will round it to either 0 or 1 for binary classification (positive for 1, negative for 0).\n",
        "In this first example, the label is rounded to 1, meaning that the sentence is a positive review.\n",
        "You will only use `sentence` as the input; please ignore other values."
      ],
      "metadata": {
        "id": "uAVQyYWkxib6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 2.1** *(2 points)* Using space tokenizer, create the vocabulary for the training data and report the vocabulary size here. Make sure that you add an `UNK` token to the vocabulary to account for words (during inference time) that you haven't seen. See below for an example with a short text."
      ],
      "metadata": {
        "id": "2s0T6qk6x78s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# Space tokenization\n",
        "words = [word for data in sst[\"train\"] for word in data[\"sentence\"].split(\" \")]\n",
        "tokens = sorted(list(set(words)))  # to ensure reproducibility (set doesn't guarantee order)\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# Constructing vocabulary with `UNK`\n",
        "vocab = [\"PAD\", \"UNK\"] + tokens\n",
        "word2id = {word: word_id for word_id, word in enumerate(vocab)}\n",
        "print(f\"Number of vocabs: {len(vocab)}\")\n",
        "print(f\"ID of 'star': {word2id['star']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 18280\n",
            "Number of vocabs: 18282\n",
            "ID of 'star': 15904\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iCTn95_pK7i",
        "outputId": "12664f20-5696-4530-bc6c-38029756143e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 2.2** *(1 point)* Using all words in the training data will make the vocabulary very big. Reduce its size by only including words that occur at least 2 times. How does the size of the vocabulary change?"
      ],
      "metadata": {
        "id": "sqVv57zy1OZ0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Include words that occur at least 2 times\n",
        "tokens = [token for token, count in Counter(words).items() if count >= 2]\n",
        "print(f\"Number of tokens: {len(tokens)}\")\n",
        "\n",
        "# Constructing vocabulary with `UNK`\n",
        "vocab = [\"PAD\", \"UNK\"] + tokens\n",
        "word2id = {word: word_id for word_id, word in enumerate(vocab)}\n",
        "print(f\"Number of vocabs: {len(vocab)}\")\n",
        "print(f\"ID of 'star': {word2id['star']}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 8736\n",
            "Number of vocabs: 8738\n",
            "ID of 'star': 2308\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Text Classification with Multi-Layer Perceptron and Recurrent Neural Network\n",
        "\n",
        "You can now use the vocabulary constructed from the training data to create an embedding matrix. You will use the embedding matrix to map each input sequence of tokens to a list of embedding vectors. One of the simplest baseline is to fix the input length (with truncation of padding), flatten the word embeddings, apply a linear transformation followed by an activation, and finally classify the output into the two classes: "
      ],
      "metadata": {
        "id": "nDQbmM3W2Im3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "words = [word.lower() for data in sst[\"train\"] for word in re.split(r\"\\W+\", data[\"sentence\"]) if word]\n",
        "tokens = [token for token, count in Counter(words).items() if count >= 2]\n",
        "vocab = [\"pad\", \"unk\"] + tokens\n",
        "word2id = {word: word_id for word_id, word in enumerate(vocab)}"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "def tokenize(sentence, length):\n",
        "    tokens = [word for word in re.split(r\"\\W+\", sentence.lower()) if word]\n",
        "    tokens = (tokens + [\"pad\"] * (length - len(tokens)))[:length]\n",
        "    return tokens\n",
        "\n",
        "def binarize(label):\n",
        "    return 1 if label > 0.5 else 0\n",
        "\n",
        "def preprocess(sentences, labels, length):\n",
        "    x = [tokenize(sentence, length) for sentence in sentences]\n",
        "    y = [binarize(label) for label in labels]\n",
        "    return x, y"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see the examples:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "length = 8\n",
        "input_sentence = \"What a nice day!\"\n",
        "input_tensor = torch.LongTensor([[\n",
        "    word2id.get(token, 1) for token in tokenize(input_sentence, length)\n",
        "]])  # the first dimension is minibatch size\n",
        "print(f\"{input_sentence} -> {input_tensor}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What a nice day! -> tensor([[266,  18, 297, 591,   0,   0,   0,   0]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "source": [
        "class ModelBase(nn.Sequential):\n",
        "    def __init__(self, hiddens, dim, embed=None):\n",
        "        super().__init__(*(([] if embed is None else [nn.Embedding(len(embed), dim)])\n",
        "                         + (hiddens if type(hiddens) is list else [hiddens])\n",
        "                         + [nn.ReLU(), nn.Linear(dim, 2)]))"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "source": [
        "class MLPModel(ModelBase):\n",
        "    def __init__(self, dim, length, embed=None):\n",
        "        super().__init__([\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(dim * length, dim),\n",
        "        ], dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "torch.manual_seed(19)\n",
        "baseline = MLPModel(dim=3, length=length, embed=vocab)  # dim is usually bigger, e.g. 128\n",
        "logits = baseline(input_tensor)\n",
        "print(f\"softmax logits: {F.softmax(logits, dim=1).detach()}\")  # probability for each class"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "softmax logits: tensor([[0.6985, 0.3015]])\n"
          ]
        }
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vUmITCFqMit",
        "outputId": "69762f39-e932-457c-f06b-bf78047a6c50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we will compute the loss, which is the negative log probability of the input text's label being the target label (`1`), which in fact turns out to be equivalent to the [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) between the probability distribution and a one-hot distribution of the target label (note that we use `logits` instead of `softmax(logits)` as the input to the cross entropy, which allow us to avoid numerical instability). "
      ],
      "metadata": {
        "id": "G9UuMWCG9YNs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "ce = nn.CrossEntropyLoss()\n",
        "label = torch.LongTensor([1])  # The ground truth label for \"What a nice day!\" is positive.\n",
        "loss = ce(logits, label)  # Loss, a.k.a L\n",
        "print(f\"loss: {loss.detach():.6f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss: 1.198835\n"
          ]
        }
      ],
      "metadata": {
        "id": "6nxgYNzQqaPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ce04c23-b72a-4127-fa07-3d796fc75366"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the loss defined, only one step remains! We compute the gradients of parameters with respective to the loss and update. Fortunately, PyTorch does this for us in a very convenient way. Note that we used only one example to update the model, which is basically a Stochastic Gradient Descent (SGD) with minibatch size of 1. A recommended minibatch size in this exercise is at least 16. It is also recommended that you reuse your training data at least 10 times (i.e. 10 *epochs*)."
      ],
      "metadata": {
        "id": "IKR99jZ2-wZW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "source": [
        "optimizer = torch.optim.SGD(baseline.parameters(), lr=0.1)\n",
        "optimizer.zero_grad()  # reset process\n",
        "loss.backward()  # compute gradients\n",
        "optimizer.step()  # update parameters"
      ],
      "outputs": [],
      "metadata": {
        "id": "w8JjhgQ071d6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you have done this, all weight parameters will have `grad` attributes that contain their gradients with respect to the loss."
      ],
      "metadata": {
        "id": "t8M0fhFf_LbG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "source": [
        "print(baseline[2].weight.grad)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000],\n",
            "        [-0.0022, -0.0209, -0.0229,  0.0020, -0.0056, -0.0060,  0.0074,  0.0346,\n",
            "          0.0035,  0.0151,  0.0065,  0.0169,  0.0153, -0.0066, -0.0034,  0.0153,\n",
            "         -0.0066, -0.0034,  0.0153, -0.0066, -0.0034,  0.0153, -0.0066, -0.0034],\n",
            "        [-0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000,  0.0000,\n",
            "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0000, -0.0000,  0.0000,\n",
            "         -0.0000, -0.0000,  0.0000, -0.0000, -0.0000,  0.0000, -0.0000, -0.0000]])\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define the helper class to train the model and get the data loaders"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "class Trainer:\n",
        "    def __init__(self, model, optimizer):\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        \n",
        "    def train_epoch(self, dataloader):\n",
        "        self.model.train()\n",
        "        moving_loss = None\n",
        "        for x_batch, y_batch in dataloader:\n",
        "            logits = self.model(x_batch)\n",
        "            loss = self.criterion(logits, y_batch).detach()\n",
        "            moving_loss = (loss.detach() if moving_loss is None else\n",
        "                           0.2 * moving_loss + 0.8 * loss.detach())\n",
        "            self.optimizer.zero_grad()  # reset process\n",
        "            loss.backward()  # compute gradients\n",
        "            self.optimizer.step()  # update parameters\n",
        "        return moving_loss\n",
        "    \n",
        "    def test_epoch(self, dataloader):\n",
        "        with torch.no_grad():\n",
        "            self.model.eval()\n",
        "            correct, total = 0, 0\n",
        "            for x_batch, y_batch in dataloader:\n",
        "                logits = self.model(x_batch)\n",
        "                y_pred = torch.argmax(logits, dim=1)\n",
        "                correct += torch.sum(y_pred == y_batch).item()\n",
        "                total += len(y_batch)\n",
        "            return correct * 100 / total\n",
        "        \n",
        "    def train(self, train_loader, valid_loader, epochs=10, print_every=10):\n",
        "        print(f\"{self.model.__class__.__name__}\")\n",
        "        best_acc = 0.\n",
        "        for i in range(epochs):\n",
        "            train_loss = self.train_epoch(train_loader)\n",
        "            if (i + 1) % print_every == 0:\n",
        "                train_acc = self.test_epoch(train_loader)\n",
        "                valid_acc = self.test_epoch(valid_loader)\n",
        "                print(f\"Epoch {i+1:3d}: Train Loss {train_loss:.6f}, \"\n",
        "                      f\"Train Acc: {train_acc:.2f}, Valid Acc: {valid_acc:.2f}\")\n",
        "                best_acc = max(best_acc, valid_acc)\n",
        "        print(f\"Best Valid Acc: {best_acc:.2f}%\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "def get_dataloader(x, y, x_trans, y_trans, batch_size, shuffle):\n",
        "    dataset = TensorDataset(torch.vstack([x_trans(x_item) for x_item in x]),\n",
        "                            torch.cat([y_trans(y_item) for y_item in y]))\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    return dataloader"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "length = 32\n",
        "batch_size = 128\n",
        "\n",
        "unk_id = word2id[\"unk\"]\n",
        "x2long = lambda x_item: torch.LongTensor([word2id.get(token, unk_id) for token in x_item]).to(device)\n",
        "y2long = lambda y_item: torch.LongTensor([y_item]).to(device)\n",
        "\n",
        "sst_train, sst_valid = sst[\"train\"], sst[\"validation\"]\n",
        "\n",
        "x_train, y_train = preprocess(sst_train[\"sentence\"], sst_train[\"label\"], length)\n",
        "x_valid, y_valid = preprocess(sst_valid[\"sentence\"], sst_valid[\"label\"], length)\n",
        "\n",
        "train_loader = get_dataloader(x_train, y_train, x2long, y2long, batch_size, shuffle=True)\n",
        "valid_loader = get_dataloader(x_valid, y_valid, x2long, y2long, batch_size, shuffle=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 3.1** *(2 points)* Properly train a MLP baseline model on SST and report the model's accuracy on the dev data."
      ],
      "metadata": {
        "id": "W-O9D26U_aEB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "source": [
        "torch.manual_seed(19)\n",
        "model = MLPModel(dim=64, length=length, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPModel\n",
            "Epoch  10: Train Loss 0.329118, Train Acc: 89.95, Valid Acc: 56.86\n",
            "Epoch  20: Train Loss 0.153379, Train Acc: 97.10, Valid Acc: 58.49\n",
            "Epoch  30: Train Loss 0.083661, Train Acc: 99.18, Valid Acc: 59.22\n",
            "Epoch  40: Train Loss 0.044256, Train Acc: 99.88, Valid Acc: 60.94\n",
            "Epoch  50: Train Loss 0.043314, Train Acc: 99.95, Valid Acc: 63.40\n",
            "Epoch  60: Train Loss 0.047403, Train Acc: 99.64, Valid Acc: 65.67\n",
            "Epoch  70: Train Loss 0.032097, Train Acc: 99.82, Valid Acc: 64.58\n",
            "Epoch  80: Train Loss 0.048647, Train Acc: 99.81, Valid Acc: 65.21\n",
            "Epoch  90: Train Loss 0.014697, Train Acc: 99.96, Valid Acc: 67.39\n",
            "Epoch 100: Train Loss 0.012125, Train Acc: 99.95, Valid Acc: 67.03\n",
            "Best Valid Acc: 67.39%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 3.2** *(2 points)* Implement a recurrent neural network (without using PyTorch's RNN module) where the output of the linear layer not only depends on the current input but also the previous output. Report the model's accuracy on the dev data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RNNCell(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "        self.V = nn.Linear(hidden_features, hidden_features, bias=False)\n",
        "        self.U = nn.Linear(in_features, hidden_features)\n",
        "        self.tanh = nn.Tanh()\n",
        "        \n",
        "    def forward(self, h, x):\n",
        "        h = self.tanh(self.V(h) + self.U(x))\n",
        "        return h"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, num_layers):\n",
        "        super().__init__()\n",
        "        self.hidden_features = hidden_features\n",
        "        self.layers = nn.ModuleList([\n",
        "            RNNCell((in_features if i == 0 else hidden_features), hidden_features)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.W = nn.Linear(hidden_features, hidden_features)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        h = [torch.zeros((x.shape[0], self.hidden_features), device=x.device)\n",
        "             for _ in range(len(self.layers))]\n",
        "        for t in range(x.shape[1]):\n",
        "            xt = x[:, t, :]\n",
        "            for i, layer in enumerate(self.layers):\n",
        "                h[i] = layer(h[i], h[i - 1] if i > 0 else xt)\n",
        "        o = self.W(h[-1])\n",
        "        return o"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class RNNModel(ModelBase):\n",
        "    def __init__(self, dim, embed=None):\n",
        "        super().__init__(RNN(dim, dim, 1), dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "source": [
        "torch.manual_seed(19)\n",
        "model = RNNModel(dim=64, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel\n",
            "Epoch  10: Train Loss 0.646151, Train Acc: 53.25, Valid Acc: 50.32\n",
            "Epoch  20: Train Loss 0.641882, Train Acc: 54.38, Valid Acc: 51.23\n",
            "Epoch  30: Train Loss 0.625008, Train Acc: 56.03, Valid Acc: 48.05\n",
            "Epoch  40: Train Loss 0.587662, Train Acc: 59.68, Valid Acc: 48.96\n",
            "Epoch  50: Train Loss 0.483745, Train Acc: 66.75, Valid Acc: 49.50\n",
            "Epoch  60: Train Loss 0.167913, Train Acc: 95.97, Valid Acc: 51.50\n",
            "Epoch  70: Train Loss 0.025004, Train Acc: 99.08, Valid Acc: 51.41\n",
            "Epoch  80: Train Loss 0.131768, Train Acc: 99.15, Valid Acc: 51.68\n",
            "Epoch  90: Train Loss 0.028336, Train Acc: 99.52, Valid Acc: 51.50\n",
            "Epoch 100: Train Loss 0.049282, Train Acc: 98.89, Valid Acc: 52.86\n",
            "Best Valid Acc: 52.86%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 3.3** *(2 points)* Show that the cross entropy computed above is equivalent to the negative log likelihood of the probability distribution."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The negative log likelihood $\\text{NLL}$ of the probability distribution $p$ given parameters $\\theta$ is,\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        \\text{NLL}(\\theta | x)\n",
        "        &= -\\frac{1}{N} \\log \\mathbb{P}_\\theta(X=x) \\\\\n",
        "        &= -\\frac{1}{N} \\log \\prod_{x\\in\\mathcal{X}} q_\\theta(x)^{Np(x)} \\\\\n",
        "        &= -\\frac{1}{N} \\sum_{x\\in\\mathcal{X}} \\log q_\\theta(x)^{Np(x)} \\\\\n",
        "        &= -\\sum_{x\\in\\mathcal{X}} p(x) \\log q_\\theta(x) \\\\\n",
        "        &= H(p, q)\n",
        "    \\end{align*}\n",
        "$$\n",
        "where $H(p, q)$ is the cross entropy.\n",
        "So, the cross entropy is equivalent to the negative log likelihood."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 3.4 (bonus)** *(1 points)* Why is it numerically unstable if you compute log on top of softmax?"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The softmax is defined as:\n",
        "$$\n",
        "    \\sigma(\\textbf{z})_k = \\frac{e^{z_k}}{\\sum_{i=1}^K e^{z_i}} \\qquad\n",
        "    \\text{for } k = 1, 2, \\cdots, K \\text{ and } \\textbf{z} = (z_1, \\cdots, z_k) \\in \\mathbb{R}^K.\n",
        "$$\n",
        "Let $c$ is the maximum value of elements in $\\mathbf{z}$, which means,\n",
        "$$\n",
        "    c = \\max_{1 \\le i \\le K} z_i.\n",
        "$$\n",
        "Then, $\\mathbf{z}$ can be written as:\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        \\textbf{z}\n",
        "        &= (z_1, \\cdots, z_k) \\\\\n",
        "        &= \\left(c + (z_1 - c), \\cdots, c + (z_k - c)\\right).\n",
        "    \\end{align*}\n",
        "$$\n",
        "The log of the softmax $\\textbf{z}$ can be computed as:\n",
        "$$\n",
        "    \\begin{align*}\n",
        "        \\log\\sigma(\\textbf{z})\n",
        "        &= \\log\\left(\\frac{e^{z_1}}{\\sum_{i=1}^K e^{z_i}}, \\cdots, \\frac{e^{z_K}}{\\sum_{i=1}^K e^{z_i}}\\right) \\\\\n",
        "        &= \\log\\left(\\frac{e^{z_1 - c}}{\\sum_{i=1}^K e^{z_i - c}}, \\cdots, \\frac{e^{z_K - c}}{\\sum_{i=1}^K e^{z_i - c}}\\right) \\\\\n",
        "        &= \\left(\\log\\frac{e^{z_1 - c}}{\\sum_{i=1}^K e^{z_i - c}}, \\cdots, \\log\\frac{e^{z_K - c}}{\\sum_{i=1}^K e^{z_i - c}}\\right).\n",
        "    \\end{align*}\n",
        "$$\n",
        "Note that $\\sum_{i=1}^K e^{z_i - c} \\ge 1$ because there exists $e^{c-c} = e^0 = 1$ when $z_k = c$. \\\n",
        "Now, let $d$ is the minimum value of elements in $\\mathbf{z}$, which means,\n",
        "$$\n",
        "    d = \\min_{1 \\le j \\le K} z_j.\n",
        "$$\n",
        "Then, as $d - c \\to -\\infty$, $e^{d-c} \\to 0$ and $\\frac{e^{d-c}}{\\sum_{i=1}^K e^{z_i - c}} \\to 0$.\n",
        "This gives $\\log\\frac{e^{d-c}}{\\sum_{i=1}^K e^{z_i - c}} \\to -\\infty$ in the result of the log of the softmax $\\textbf{z}$, which makes numerically unstable."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Classification with LSTM and Dropout\n",
        "\n",
        "Replace your RNN module with an LSTM module. See Lecture slides 05 and 06 for the formal definition of LSTMs. \n",
        "\n",
        "You will also use Dropout, which randomly makes each dimension zero with the probability of `p` and scale it by `1/(1-p)` if it is not zero during training. Put it either at the input or the output of the LSTM to prevent it from overfitting."
      ],
      "metadata": {
        "id": "PBLmpKJRAd1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "        self.Vf = nn.Linear(hidden_features, hidden_features, bias=False)\n",
        "        self.Vi = nn.Linear(hidden_features, hidden_features, bias=False)\n",
        "        self.Vo = nn.Linear(hidden_features, hidden_features, bias=False)\n",
        "        self.Vc = nn.Linear(hidden_features, hidden_features, bias=False)\n",
        "        self.Uf = nn.Linear(in_features, hidden_features)\n",
        "        self.Ui = nn.Linear(in_features, hidden_features)\n",
        "        self.Uo = nn.Linear(in_features, hidden_features)\n",
        "        self.Uc = nn.Linear(in_features, hidden_features)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self, h, c, x):\n",
        "        f = self.sigmoid(self.Vf(h) + self.Uf(x))\n",
        "        i = self.sigmoid(self.Vi(h) + self.Ui(x))\n",
        "        o = self.sigmoid(self.Vo(h) + self.Uo(x))\n",
        "        c_ = self.tanh(self.Vc(h) + self.Uc(x))\n",
        "        c = f * c + i * c_\n",
        "        h = o * self.tanh(c)\n",
        "        return h, c"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, dropout=0, num_layers=1, bidirectional=False):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_features = hidden_features\n",
        "        self.layers = nn.ModuleList([\n",
        "            LSTMCell((hidden_features if i > 0 else in_features), hidden_features)\n",
        "            for i in range(num_layers)\n",
        "        ])\n",
        "        self.dropouts = nn.ModuleList([\n",
        "            nn.Dropout(dropout) if dropout > 0 else nn.Identity()\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.W = nn.Linear(hidden_features * (2 if bidirectional else 1), hidden_features)\n",
        "            \n",
        "    def forward(self, x):\n",
        "        x_seq = range(x.shape[1])\n",
        "        hs = []\n",
        "        for seq in [x_seq, reversed(x_seq)] if self.bidirectional else [x_seq]:\n",
        "            h = [torch.zeros((x.shape[0], self.hidden_features), device=x.device)\n",
        "                for _ in range(len(self.layers))]\n",
        "            c = [torch.zeros((x.shape[0], self.hidden_features), device=x.device)\n",
        "                for _ in range(len(self.layers))]\n",
        "            for t in seq:\n",
        "                xt = x[:, t, :]\n",
        "                for i in range(len(self.layers)):\n",
        "                    h_ = (self.dropouts[i - 1](h[i - 1]) if i > 0 else xt)\n",
        "                    h[i], c[i] = self.layers[i](h[i], c[i], h_)\n",
        "            hs.append(h[-1])\n",
        "        h = torch.cat(hs, dim=1)\n",
        "        o = self.W(self.dropouts[-1](h))\n",
        "        return o"
      ],
      "outputs": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8H0F-csCCk0",
        "outputId": "b9639721-dc91-48dd-b82c-3b179bf04742"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 4.1** *(3 points)* Implement and use LSTM (without using PyTorch's LSTM module) instead of vanilla RNN. Report the accuracy on the dev data."
      ],
      "metadata": {
        "id": "G6boSxKjC4Mw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "class LSTMModel(ModelBase):\n",
        "    def __init__(self, dim, embed=None):\n",
        "        super().__init__(LSTM(dim, dim), dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMModel(dim=64, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel\n",
            "Epoch  10: Train Loss 0.689929, Train Acc: 50.33, Valid Acc: 49.32\n",
            "Epoch  20: Train Loss 0.687881, Train Acc: 50.55, Valid Acc: 49.86\n",
            "Epoch  30: Train Loss 0.691242, Train Acc: 51.64, Valid Acc: 50.23\n",
            "Epoch  40: Train Loss 0.683999, Train Acc: 53.03, Valid Acc: 50.23\n",
            "Epoch  50: Train Loss 0.322293, Train Acc: 91.71, Valid Acc: 73.12\n",
            "Epoch  60: Train Loss 0.160475, Train Acc: 97.34, Valid Acc: 71.39\n",
            "Epoch  70: Train Loss 0.128818, Train Acc: 97.83, Valid Acc: 72.03\n",
            "Epoch  80: Train Loss 0.186474, Train Acc: 97.87, Valid Acc: 72.12\n",
            "Epoch  90: Train Loss 0.069050, Train Acc: 98.44, Valid Acc: 71.03\n",
            "Epoch 100: Train Loss 0.144623, Train Acc: 97.95, Valid Acc: 72.12\n",
            "Best Valid Acc: 73.12%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 4.2** *(2 points)* Use Dropout on LSTM (either at input or output). Report the accuracy on the dev data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "source": [
        "class LSTMDropoutModel(ModelBase):\n",
        "    def __init__(self, dim, dropout, embed=None):\n",
        "        super().__init__(LSTM(dim, dim, dropout), dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMDropoutModel(dim=64, dropout=0.3, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMDropoutModel\n",
            "Epoch  10: Train Loss 0.691265, Train Acc: 50.33, Valid Acc: 49.32\n",
            "Epoch  20: Train Loss 0.687724, Train Acc: 50.35, Valid Acc: 49.32\n",
            "Epoch  30: Train Loss 0.691111, Train Acc: 51.09, Valid Acc: 50.23\n",
            "Epoch  40: Train Loss 0.694906, Train Acc: 53.23, Valid Acc: 51.50\n",
            "Epoch  50: Train Loss 0.562550, Train Acc: 76.69, Valid Acc: 68.76\n",
            "Epoch  60: Train Loss 0.256842, Train Acc: 96.80, Valid Acc: 70.84\n",
            "Epoch  70: Train Loss 0.057003, Train Acc: 97.74, Valid Acc: 70.94\n",
            "Epoch  80: Train Loss 0.132960, Train Acc: 98.06, Valid Acc: 70.12\n",
            "Epoch  90: Train Loss 0.063541, Train Acc: 98.46, Valid Acc: 69.75\n",
            "Epoch 100: Train Loss 0.063577, Train Acc: 98.70, Valid Acc: 71.12\n",
            "Best Valid Acc: 71.12%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 4.3 (bonus)** *(2 points)* Consider implementing bidirectional LSTM and two layers of LSTM. Report your accuracy on dev data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "source": [
        "class LSTMBidirectionalModel(ModelBase):\n",
        "    def __init__(self, dim, embed=None):\n",
        "        super().__init__(LSTM(dim, dim, bidirectional=True), dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMBidirectionalModel(dim=64, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=7e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMBidirectionalModel\n",
            "Epoch  10: Train Loss 0.442379, Train Acc: 81.06, Valid Acc: 69.39\n",
            "Epoch  20: Train Loss 0.239456, Train Acc: 93.59, Valid Acc: 69.30\n",
            "Epoch  30: Train Loss 0.089283, Train Acc: 98.90, Valid Acc: 69.12\n",
            "Epoch  40: Train Loss 0.108936, Train Acc: 95.42, Valid Acc: 69.85\n",
            "Epoch  50: Train Loss 0.012590, Train Acc: 99.92, Valid Acc: 70.84\n",
            "Epoch  60: Train Loss 0.042250, Train Acc: 99.31, Valid Acc: 70.39\n",
            "Epoch  70: Train Loss 0.015697, Train Acc: 99.85, Valid Acc: 71.12\n",
            "Epoch  80: Train Loss 0.022438, Train Acc: 99.92, Valid Acc: 70.03\n",
            "Epoch  90: Train Loss 0.012001, Train Acc: 99.96, Valid Acc: 71.75\n",
            "Epoch 100: Train Loss 0.057801, Train Acc: 99.64, Valid Acc: 70.75\n",
            "Best Valid Acc: 71.75%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "source": [
        "class LSTMTwoLayerModel(ModelBase):\n",
        "    def __init__(self, dim, embed=None):\n",
        "        super().__init__(LSTM(dim, dim, num_layers=2), dim, embed)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMTwoLayerModel(dim=64, embed=vocab).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4),\n",
        ").train(train_loader, valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMTwoLayerModel\n",
            "Epoch  10: Train Loss 0.665834, Train Acc: 54.83, Valid Acc: 51.04\n",
            "Epoch  20: Train Loss 0.298917, Train Acc: 92.11, Valid Acc: 73.48\n",
            "Epoch  30: Train Loss 0.076886, Train Acc: 97.75, Valid Acc: 72.30\n",
            "Epoch  40: Train Loss 0.046536, Train Acc: 98.09, Valid Acc: 70.84\n",
            "Epoch  50: Train Loss 0.013854, Train Acc: 99.10, Valid Acc: 70.30\n",
            "Epoch  60: Train Loss 0.031739, Train Acc: 99.16, Valid Acc: 68.48\n",
            "Epoch  70: Train Loss 0.006761, Train Acc: 99.60, Valid Acc: 71.30\n",
            "Epoch  80: Train Loss 0.005833, Train Acc: 99.73, Valid Acc: 70.75\n",
            "Epoch  90: Train Loss 0.011402, Train Acc: 99.77, Valid Acc: 68.85\n",
            "Epoch 100: Train Loss 0.031055, Train Acc: 99.78, Valid Acc: 69.94\n",
            "Best Valid Acc: 73.48%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Pretrained Word Vectors\n",
        "The last step is to use pretrained vocabulary and word vectors. The prebuilt vocabulary will replace the vocabulary you built with SST training data, and the word vectors will replace the embedding vectors. You will observe the power of leveraging self-supservised pretrained models."
      ],
      "metadata": {
        "id": "855DrT78DXps"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **Problem 5.1 (bonus)** *(2 points)* Go to https://nlp.stanford.edu/projects/glove/ and download `glove.6B.zip`. Use these pretrained word vectors to replace word embeddings in your model from 4.2. Report the model's accuracy on the dev data."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "source": [
        "from urllib import request\n",
        "import zipfile\n",
        "\n",
        "if not os.path.exists(\"glove/glove.6B.100d.txt\"):\n",
        "    os.makedirs(\"glove\", exist_ok=True)\n",
        "    print(\"Downloading glove.6B.zip\")\n",
        "    request.urlretrieve(\"http://nlp.stanford.edu/data/glove.6B.zip\", \"glove/glove.6B.zip\")\n",
        "    with zipfile.ZipFile(\"glove.6B.zip\", \"r\") as zipf:\n",
        "        zipf.extractall(\"glove\")"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "source": [
        "dim = 50\n",
        "\n",
        "with open(f\"glove/glove.6B.{dim}d.txt\", \"r\") as glove:\n",
        "    glove_map = map(lambda line: line.split(), glove.readlines())\n",
        "    word2vec = {word: list(map(float, vec)) for word, *vec in glove_map}\n",
        "\n",
        "unk_vec = word2vec[\"unk\"]\n",
        "x2vec = lambda x_item: torch.Tensor([[word2vec.get(token, unk_vec) for token in x_item]]).to(device)\n",
        "\n",
        "glove_train_loader = get_dataloader(x_train, y_train, x2vec, y2long, batch_size, shuffle=True)\n",
        "glove_valid_loader = get_dataloader(x_valid, y_valid, x2vec, y2long, batch_size, shuffle=False)"
      ],
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "source": [
        "torch.manual_seed(19)\n",
        "model = MLPModel(dim=dim, length=length).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=3e-3),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPModel\n",
            "Epoch  10: Train Loss 0.404025, Train Acc: 78.82, Valid Acc: 66.03\n",
            "Epoch  20: Train Loss 0.377533, Train Acc: 86.39, Valid Acc: 67.57\n",
            "Epoch  30: Train Loss 0.323054, Train Acc: 91.33, Valid Acc: 65.30\n",
            "Epoch  40: Train Loss 0.191065, Train Acc: 94.53, Valid Acc: 67.94\n",
            "Epoch  50: Train Loss 0.130630, Train Acc: 96.52, Valid Acc: 67.30\n",
            "Epoch  60: Train Loss 0.085130, Train Acc: 98.43, Valid Acc: 65.85\n",
            "Epoch  70: Train Loss 0.118676, Train Acc: 98.50, Valid Acc: 66.94\n",
            "Epoch  80: Train Loss 0.128824, Train Acc: 98.67, Valid Acc: 64.76\n",
            "Epoch  90: Train Loss 0.083241, Train Acc: 99.44, Valid Acc: 65.76\n",
            "Epoch 100: Train Loss 0.055838, Train Acc: 99.57, Valid Acc: 65.67\n",
            "Best Valid Acc: 67.94%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "source": [
        "torch.manual_seed(19)\n",
        "model = RNNModel(dim=dim).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=3e-4),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNNModel\n",
            "Epoch  10: Train Loss 0.694202, Train Acc: 51.56, Valid Acc: 49.95\n",
            "Epoch  20: Train Loss 0.618280, Train Acc: 68.68, Valid Acc: 68.94\n",
            "Epoch  30: Train Loss 0.553833, Train Acc: 70.38, Valid Acc: 68.21\n",
            "Epoch  40: Train Loss 0.598823, Train Acc: 71.89, Valid Acc: 68.94\n",
            "Epoch  50: Train Loss 0.547611, Train Acc: 72.53, Valid Acc: 70.21\n",
            "Epoch  60: Train Loss 0.503621, Train Acc: 69.22, Valid Acc: 65.12\n",
            "Epoch  70: Train Loss 0.490134, Train Acc: 73.72, Valid Acc: 68.66\n",
            "Epoch  80: Train Loss 0.583035, Train Acc: 75.49, Valid Acc: 71.57\n",
            "Epoch  90: Train Loss 0.463026, Train Acc: 76.42, Valid Acc: 71.48\n",
            "Epoch 100: Train Loss 0.536338, Train Acc: 77.00, Valid Acc: 72.21\n",
            "Best Valid Acc: 72.21%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMModel(dim=dim).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=3e-4),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMModel\n",
            "Epoch  10: Train Loss 0.591228, Train Acc: 69.71, Valid Acc: 70.66\n",
            "Epoch  20: Train Loss 0.500636, Train Acc: 73.01, Valid Acc: 72.84\n",
            "Epoch  30: Train Loss 0.538379, Train Acc: 75.67, Valid Acc: 74.11\n",
            "Epoch  40: Train Loss 0.459484, Train Acc: 77.38, Valid Acc: 72.84\n",
            "Epoch  50: Train Loss 0.471294, Train Acc: 79.07, Valid Acc: 74.57\n",
            "Epoch  60: Train Loss 0.413298, Train Acc: 80.86, Valid Acc: 74.39\n",
            "Epoch  70: Train Loss 0.320830, Train Acc: 82.16, Valid Acc: 74.75\n",
            "Epoch  80: Train Loss 0.269459, Train Acc: 82.65, Valid Acc: 74.11\n",
            "Epoch  90: Train Loss 0.381291, Train Acc: 83.95, Valid Acc: 73.93\n",
            "Epoch 100: Train Loss 0.364630, Train Acc: 85.90, Valid Acc: 74.66\n",
            "Best Valid Acc: 74.75%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMDropoutModel(dim=dim, dropout=0.3).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=3e-4),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMDropoutModel\n",
            "Epoch  10: Train Loss 0.637870, Train Acc: 66.53, Valid Acc: 67.94\n",
            "Epoch  20: Train Loss 0.516867, Train Acc: 72.88, Valid Acc: 73.57\n",
            "Epoch  30: Train Loss 0.535345, Train Acc: 75.90, Valid Acc: 73.66\n",
            "Epoch  40: Train Loss 0.429677, Train Acc: 75.12, Valid Acc: 71.57\n",
            "Epoch  50: Train Loss 0.451197, Train Acc: 78.99, Valid Acc: 73.93\n",
            "Epoch  60: Train Loss 0.438715, Train Acc: 80.55, Valid Acc: 74.21\n",
            "Epoch  70: Train Loss 0.313589, Train Acc: 82.14, Valid Acc: 73.48\n",
            "Epoch  80: Train Loss 0.332740, Train Acc: 83.24, Valid Acc: 73.57\n",
            "Epoch  90: Train Loss 0.348202, Train Acc: 82.95, Valid Acc: 73.93\n",
            "Epoch 100: Train Loss 0.366501, Train Acc: 85.76, Valid Acc: 73.12\n",
            "Best Valid Acc: 74.21%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMBidirectionalModel(dim=dim).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=3e-4),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMBidirectionalModel\n",
            "Epoch  10: Train Loss 0.658024, Train Acc: 71.51, Valid Acc: 68.39\n",
            "Epoch  20: Train Loss 0.486861, Train Acc: 75.87, Valid Acc: 72.93\n",
            "Epoch  30: Train Loss 0.438417, Train Acc: 78.20, Valid Acc: 75.57\n",
            "Epoch  40: Train Loss 0.420470, Train Acc: 80.09, Valid Acc: 75.57\n",
            "Epoch  50: Train Loss 0.390630, Train Acc: 81.66, Valid Acc: 75.02\n",
            "Epoch  60: Train Loss 0.370695, Train Acc: 84.09, Valid Acc: 75.39\n",
            "Epoch  70: Train Loss 0.308943, Train Acc: 85.01, Valid Acc: 73.57\n",
            "Epoch  80: Train Loss 0.270784, Train Acc: 87.96, Valid Acc: 74.02\n",
            "Epoch  90: Train Loss 0.263489, Train Acc: 89.36, Valid Acc: 73.39\n",
            "Epoch 100: Train Loss 0.235636, Train Acc: 91.00, Valid Acc: 72.66\n",
            "Best Valid Acc: 75.57%\n"
          ]
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "source": [
        "torch.manual_seed(19)\n",
        "model = LSTMTwoLayerModel(dim=dim).to(device)\n",
        "Trainer(\n",
        "    model, torch.optim.Adam(model.parameters(), lr=3e-4),\n",
        ").train(glove_train_loader, glove_valid_loader, epochs=100)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTMTwoLayerModel\n",
            "Epoch  10: Train Loss 0.577003, Train Acc: 70.49, Valid Acc: 71.93\n",
            "Epoch  20: Train Loss 0.490339, Train Acc: 74.41, Valid Acc: 71.75\n",
            "Epoch  30: Train Loss 0.445460, Train Acc: 76.98, Valid Acc: 74.30\n",
            "Epoch  40: Train Loss 0.494168, Train Acc: 78.45, Valid Acc: 73.75\n",
            "Epoch  50: Train Loss 0.397014, Train Acc: 80.43, Valid Acc: 74.21\n",
            "Epoch  60: Train Loss 0.377189, Train Acc: 81.51, Valid Acc: 72.84\n",
            "Epoch  70: Train Loss 0.286008, Train Acc: 81.51, Valid Acc: 73.84\n",
            "Epoch  80: Train Loss 0.362238, Train Acc: 84.68, Valid Acc: 73.75\n",
            "Epoch  90: Train Loss 0.342750, Train Acc: 84.84, Valid Acc: 73.02\n",
            "Epoch 100: Train Loss 0.341504, Train Acc: 83.88, Valid Acc: 73.12\n",
            "Best Valid Acc: 74.30%\n"
          ]
        }
      ],
      "metadata": {}
    }
  ]
}